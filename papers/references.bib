
@misc{bakkerAIArchivesUsing2020,
  title = {{{AI}} for {{Archives}}: {{Using Facial Recognition}} to {{Enhance Metadata}}},
  shorttitle = {{{AI}} for {{Archives}}},
  author = {Bakker, Rebecca and Rowan, K. and Hu, Liting and Guan, Boyuan and Liu, Pinchao and Li, Z. and He, Ruizhe and Monge, Christine},
  year = {2020},
  abstract = {Semantic Scholar extracted view of \&quot;AI for Archives: Using Facial Recognition to Enhance Metadata\&quot; by Rebecca Bakker et al.},
  howpublished = {/paper/AI-for-Archives\%3A-Using-Facial-Recognition-to-Bakker-Rowan/f8290bfdbd6aa047bb1a52de07371c4f97e50070},
  journal = {undefined},
  language = {en}
}

@misc{HarvardartmuseumsDatagrinder2021,
  title = {Harvardartmuseums/Data-Grinder},
  year = {2021},
  month = feb,
  abstract = {Processes an image through several computer vision services},
  howpublished = {Harvard Art Museums},
  keywords = {computer-vision}
}

@misc{hindleAutomatedImageAnalysis2017,
  title = {Automated Image Analysis with {{IIIF}}},
  author = {Hindle, Adrian},
  year = {2017},
  month = jun,
  abstract = {Using Artificial Intelligence for bulk image analysis},
  howpublished = {https://blog.cogapp.com/automated-image-analysis-with-iiif-6594ff5b2b32},
  journal = {Medium},
  language = {en}
}

@misc{ImaGo,
  title = {{{imaGo}}},
  howpublished = {http://imago.unibo.it/search}
}

@article{leeNewspaperNavigatorDataset2020a,
  title = {The {{Newspaper Navigator Dataset}}: {{Extracting And Analyzing Visual Content}} from 16 {{Million Historic Newspaper Pages}} in {{Chronicling America}}},
  shorttitle = {The {{Newspaper Navigator Dataset}}},
  author = {Lee, B. and Mears, Jaime and Jakeway, Eileen and Ferriter, Meghan M. and Adams, C. and Yarasavage, Nathan and Thomas, D. and Zwaard, Kate and Weld, Daniel S.},
  year = {2020},
  abstract = {Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.},
  file = {/Users/dvanstrien/Zotero/storage/2I56S5DX/Lee et al. - 2020 - The Newspaper Navigator Dataset Extracting And An.pdf},
  journal = {ArXiv}
}

@article{lincolnCAMPIComputerAidedMetadata2020,
  title = {{{CAMPI}}: {{Computer}}-{{Aided Metadata Generation}} for {{Photo}} Archives {{Initiative}}},
  shorttitle = {{{CAMPI}}},
  author = {Lincoln, Matthew and Corrin, Julia and Davis, Emily and Weingart, Scott B.},
  year = {2020},
  month = oct,
  publisher = {{Carnegie Mellon University}},
  doi = {10.1184/R1/12791807.v2},
  abstract = {In the summer of 2020, CAMPI (the Computer-Aided Metadata Generation for Photoarchvies Initiative) assessed the use of computer vision in assisting in the arduous task of processing digital photograph collections. The prototype built to find duplicates and tag photos depicting similar scenes in Carnegie Mellon University Archives' General Photography Collection was successful, proving such an interface would be both feasible and useful for cultural heritage institutions with large visual collections.This whitepaper reports on the background for this project, the specific tasks, methods, and results from our software prototype, and a summary of future directions and needs for both computer vision as well as interfaces for photo archive description. We also offer a high-level technical roadmap as well as specific implementation details and reference code for our prototype.},
  file = {/Users/dvanstrien/Zotero/storage/EVEAXKSW/Lincoln et al. - 2020 - CAMPI Computer-Aided Metadata Generation for Phot.pdf},
  language = {en}
}

@misc{lincolnCAMPIComputerAidedMetadata2020a,
  title = {{{CAMPI}}: {{Computer}}-{{Aided Metadata Generation}} for {{Photo}} Archives {{Initiative}}},
  shorttitle = {{{CAMPI}}},
  author = {Lincoln, Matthew and Corrin, J. and Davis, Emily and Weingart, S.},
  year = {2020},
  abstract = {There is a big difference in the tone of color of skin between dark and light skinned people. Despite this fact, most face recognition tasks almost all classical state-ofthe-art models are trained on datasets containing an overwhelming majority of light skinned face images. It is tedious to collect a huge amount of data for dark skinned faces and train a model from scratch. In this paper, we apply transfer learning on VGGFace to check how it works on recognising dark skinned mainly Ethiopian faces. The dataset is of low quality and low resource. Our experimental results show above 95\% accuracy which indicates that transfer learning in such settings works.},
  howpublished = {/paper/Exploring-Transfer-Learning-on-Face-Recognition-of-Ali/a1075c37dc37560580dbf841d517c55864c91c05},
  journal = {undefined},
  language = {en}
}

@article{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2015},
  month = feb,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archivePrefix = {arXiv},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  file = {/Users/dvanstrien/Zotero/storage/JU98U6QT/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf},
  journal = {arXiv:1405.0312 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{offertUnderstandingPerceptualBias,
  title = {Understanding {{Perceptual Bias}} in {{Machine Vision Systems}}},
  author = {Offert, Fabian and Bell, Peter},
  pages = {11},
  abstract = {Machine vision systems based on deep convolutional neural networks are increasingly utilized in digital humanities projects, particularly in the context of art-historical and audiovisual data. As research has shown, such systems are highly susceptible to bias. We propose that this is not only due to their reliance on biased datasets but also because their perceptual topology, their specific way of representing the visual world, gives rise to a new class of bias that we call perceptual bias. Perceptual bias, we argue, affects almost all currently available ``off-the-shelf'' machine vision systems, and is thus especially relevant for digital humanities applications, which often rely on such systems for hypothesis building. We evaluate the nature and scope of perceptual bias by means of a close reading of a visual analytics technique called ``feature visualization'' and propose to understand the development of critical visual analytics techniques as an important (digital) humanities challenge, situated at the interface of computer science and visual studies.},
  file = {/Users/dvanstrien/Zotero/storage/NKN89Z4V/Offert and Bell - Understanding Perceptual Bias in Machine Vision Sy.pdf},
  language = {en}
}

@misc{PapersCodeMachine,
  title = {Papers with {{Code}} - {{Machine Learning Datasets}}},
  abstract = {139 datasets \textbullet{} 40602 papers with code.},
  file = {/Users/dvanstrien/Zotero/storage/C5SEHJ69/datasets.html},
  howpublished = {https://paperswithcode.com/datasets?q=face\&v=lst\&o=match},
  language = {en}
}

@misc{PedroproTACOTrash,
  title = {Pedropro/{{TACO}}: ðŸŒ® {{Trash Annotations}} in {{Context Dataset Toolkit}}},
  file = {/Users/dvanstrien/Zotero/storage/SX3GRFJH/TACO.html},
  howpublished = {https://github.com/pedropro/TACO}
}

@article{stacchioIMAGOFamilyPhoto2020,
  title = {{{IMAGO}}: {{A}} Family Photo Album Dataset for a Socio-Historical Analysis of the Twentieth Century},
  shorttitle = {{{IMAGO}}},
  author = {Stacchio, Lorenzo and Angeli, Alessia and Lisanti, Giuseppe and Calanca, Daniela and Marfia, Gustavo},
  year = {2020},
  month = dec,
  abstract = {Although one of the most popular practices in photography since the end of the 19th century, an increase in scholarly interest in family photo albums dates back to the early 1980s. Such collections of photos may reveal sociological and historical insights regarding specific cultures and times. They are, however, in most cases scattered among private homes and only available on paper or photographic film, thus making their analysis by academics such as historians, social-cultural anthropologists and cultural theorists very cumbersome. In this paper, we analyze the IMAGO dataset including photos belonging to family albums assembled at the University of Bologna's Rimini campus since 2004. Following a deep learning-based approach, the IMAGO dataset has offered the opportunity of experimenting with photos taken between year 1845 and year 2009, with the goals of assessing the dates and the socio-historical contexts of the images, without use of any other sources of information. Exceeding our initial expectations, such analysis has revealed its merit not only in terms of the performance of the approach adopted in this work, but also in terms of the foreseeable implications and use for the benefit of socio-historical research. To the best of our knowledge, this is the first work that moves along this path in literature.},
  archivePrefix = {arXiv},
  eprint = {2012.01955},
  eprinttype = {arxiv},
  file = {/Users/dvanstrien/Zotero/storage/SJECDQ3T/Stacchio et al. - 2020 - IMAGO A family photo album dataset for a socio-hi.pdf},
  journal = {arXiv:2012.01955 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Multimedia},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{togtokhFacialPhotoRecognition2018,
  title = {Facial {{Photo Recognition Using Deep Learning}} in {{Archival Record Management System}}},
  author = {Togtokh, Gantur and Kim, K. C. and Lee, K. W.},
  year = {2018},
  doi = {10.1007/978-981-13-9341-9_64},
  abstract = {A lot of information is stored in the archival record management system (archive), including photos and pictures. It is important to intelligently organize photos in the archive. Current approaches use face recognition technology based on deep learning to manage photos. However, due to the rapid growth of the volume of photos in the archive, face recognition processes on large photoset take lots of processing time. In addition, low resolution photo with small faces in the archive is difficult to identify and recognize. In this paper, we propose a method to identify and retrieve facial photos from the archive. In our approach, photo metadata is used for searching photos in the archive, and resolution enhancement step based on DCSCN model is used to reconstruct photos of low resolution to high resolution. Experiment shows that the proposed approach can search and retrieve facial photo quickly from large photoset and is efficient for identifying small faces of low resolution photos.}
}

@article{wangFaceXZooPyTorchToolbox2021,
  title = {{{FaceX}}-{{Zoo}}: {{A PyTorch Toolbox}} for {{Face Recognition}}},
  shorttitle = {{{FaceX}}-{{Zoo}}},
  author = {Wang, J. and Liu, Yinglu and Hu, Y. and Shi, Hailin and Mei, Tao},
  year = {2021},
  abstract = {Deep learning based face recognition has achieved significant progress in recent years. Yet, the practical model production and further research of deep face recognition are in great need of corresponding public support. For example, the production of face representation network desires a modular training scheme to consider the proper choice from various candidates of state-of-the-art backbone and training supervision subject to the real-world face recognition demand; for performance analysis and comparison, the standard and automatic evaluation with a bunch of models on multiple benchmarks will be a desired tool as well; besides, a public groundwork is welcomed for deploying the face recognition in the shape of holistic pipeline. Furthermore, there are some newly-emerged challenges, such as the masked face recognition caused by the recent world-wide COVID-19 pandemic, which draws increasing attention in practical applications. A feasible and elegant solution is to build an easy-to-use unified framework to meet the above demands. To this end, we introduce a novel open-source framework, named FaceX-Zoo, which is oriented to the research-development community of face recognition. Resorting to the highly modular and scalable design, FaceX-Zoo provides a training module with various supervisory heads and backbones towards state-of-theart face recognition, as well as a standardized evaluation module which enables to evaluate the models in most of the popular benchmarks just by editing a simple configuration. Also, a simple yet fully functional face SDK is provided for the validation and primary application of the trained models. Rather than including as many as possible of the prior techniques, we enable FaceX-Zoo to easily upgrade and extend along with the development of face related domains. The source code and models are available at: https://github.com/JDAI-CV/FaceX-Zoo. Acknowledgements Thanks to Jixuan Xu (jixuanxu@shu.edu. cn), Hang Du (duhang@shu.edu.cn), Haoran Jiang (jianghaoran@shu.edu.cn) and Hanbin Dai (daihanbin@std.uestc.edu.cn), partial works were done when they interned at JD AI Research.},
  file = {/Users/dvanstrien/OneDrive - The Alan Turing Institute/Wang et al_2021_FaceX-Zoo.pdf},
  journal = {ArXiv}
}

@misc{wangFaceXZooPyTorchToolbox2021a,
  title = {{{FaceX}}-{{Zoo}}: {{A PyTorch Toolbox}} for {{Face Recognition}}},
  shorttitle = {{{FaceX}}-{{Zoo}}},
  author = {Wang, J. and Liu, Yinglu and Hu, Y. and Shi, Hailin and Mei, Tao},
  year = {2021},
  abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
  file = {/Users/dvanstrien/OneDrive - The Alan Turing Institute/Wang et al_2021_FaceX-Zoo2.pdf;/Users/dvanstrien/Zotero/storage/XZC48294/5aa26299435bdf7db874ef1640a6c3b5a4a2c394.html},
  howpublished = {/paper/FaceNet\%3A-A-unified-embedding-for-face-recognition-Schroff-Kalenichenko/5aa26299435bdf7db874ef1640a6c3b5a4a2c394},
  journal = {undefined},
  language = {en}
}


