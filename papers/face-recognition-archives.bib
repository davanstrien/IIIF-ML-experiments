
@misc{bakkerAIArchivesUsing2020,
  title = {{{AI}} for {{Archives}}: {{Using Facial Recognition}} to {{Enhance Metadata}}},
  shorttitle = {{{AI}} for {{Archives}}},
  author = {Bakker, Rebecca and Rowan, K. and Hu, Liting and Guan, Boyuan and Liu, Pinchao and Li, Z. and He, Ruizhe and Monge, Christine},
  year = {2020},
  abstract = {Semantic Scholar extracted view of \&quot;AI for Archives: Using Facial Recognition to Enhance Metadata\&quot; by Rebecca Bakker et al.},
  howpublished = {/paper/AI-for-Archives\%3A-Using-Facial-Recognition-to-Bakker-Rowan/f8290bfdbd6aa047bb1a52de07371c4f97e50070},
  journal = {undefined},
  language = {en}
}

@article{leeNewspaperNavigatorDataset2020a,
  title = {The {{Newspaper Navigator Dataset}}: {{Extracting And Analyzing Visual Content}} from 16 {{Million Historic Newspaper Pages}} in {{Chronicling America}}},
  shorttitle = {The {{Newspaper Navigator Dataset}}},
  author = {Lee, B. and Mears, Jaime and Jakeway, Eileen and Ferriter, Meghan M. and Adams, C. and Yarasavage, Nathan and Thomas, D. and Zwaard, Kate and Weld, Daniel S.},
  year = {2020},
  abstract = {Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.},
  file = {/Users/dvanstrien/Zotero/storage/2I56S5DX/Lee et al. - 2020 - The Newspaper Navigator Dataset Extracting And An.pdf},
  journal = {ArXiv}
}

@article{lincolnCAMPIComputerAidedMetadata2020,
  title = {{{CAMPI}}: {{Computer}}-{{Aided Metadata Generation}} for {{Photo}} Archives {{Initiative}}},
  shorttitle = {{{CAMPI}}},
  author = {Lincoln, Matthew and Corrin, Julia and Davis, Emily and Weingart, Scott B.},
  year = {2020},
  month = oct,
  publisher = {{Carnegie Mellon University}},
  doi = {10.1184/R1/12791807.v2},
  abstract = {In the summer of 2020, CAMPI (the Computer-Aided Metadata Generation for Photoarchvies Initiative) assessed the use of computer vision in assisting in the arduous task of processing digital photograph collections. The prototype built to find duplicates and tag photos depicting similar scenes in Carnegie Mellon University Archives' General Photography Collection was successful, proving such an interface would be both feasible and useful for cultural heritage institutions with large visual collections.This whitepaper reports on the background for this project, the specific tasks, methods, and results from our software prototype, and a summary of future directions and needs for both computer vision as well as interfaces for photo archive description. We also offer a high-level technical roadmap as well as specific implementation details and reference code for our prototype.},
  file = {/Users/dvanstrien/Zotero/storage/EVEAXKSW/Lincoln et al. - 2020 - CAMPI Computer-Aided Metadata Generation for Phot.pdf},
  language = {en}
}

@inproceedings{togtokhFacialPhotoRecognition2018,
  title = {Facial {{Photo Recognition Using Deep Learning}} in {{Archival Record Management System}}},
  author = {Togtokh, Gantur and Kim, K. C. and Lee, K. W.},
  year = {2018},
  doi = {10.1007/978-981-13-9341-9_64},
  abstract = {A lot of information is stored in the archival record management system (archive), including photos and pictures. It is important to intelligently organize photos in the archive. Current approaches use face recognition technology based on deep learning to manage photos. However, due to the rapid growth of the volume of photos in the archive, face recognition processes on large photoset take lots of processing time. In addition, low resolution photo with small faces in the archive is difficult to identify and recognize. In this paper, we propose a method to identify and retrieve facial photos from the archive. In our approach, photo metadata is used for searching photos in the archive, and resolution enhancement step based on DCSCN model is used to reconstruct photos of low resolution to high resolution. Experiment shows that the proposed approach can search and retrieve facial photo quickly from large photoset and is efficient for identifying small faces of low resolution photos.}
}


